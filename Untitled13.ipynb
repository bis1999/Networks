{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661482ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import networkx as nx \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import  auc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e453ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_observed_set(G,alpha):\n",
    "    underlying_links = list(G.edges) # G underlying graph\n",
    "    observed_links = random.sample(underlying_links,int(len(underlying_links)*alpha)) # G' Observed net 0.8*|E|\n",
    "    \n",
    "    removed_observed_links = list(set(underlying_links) - set(observed_links)) # 0.2*|E|\n",
    "    #print(\"Number of edges in observed_links net: \",len(observed_links))\n",
    "    #print(\"Number of unseen edges in observed_links net: \",len(removed_observed_links))\n",
    "    \n",
    "    return observed_links,removed_observed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b2ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(observed_links,alpha):\n",
    "    training_links = random.sample(observed_links,int(len(observed_links)*alpha)) # G'' training net 0.64|E|\n",
    "    removed_train_links = list(set(observed_links) - set(training_links)) # 0.16|E| \n",
    "    #print(\"Number of edges in training links net: \",len(training_links))\n",
    "    #print(\"Number of unseen edges in training net: \",len(removed_train_links))\n",
    "    return training_links,removed_train_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39afffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_create(G,training_links):\n",
    "    nodes = list(G.nodes)\n",
    "    G_train = nx.Graph()\n",
    "    G_train.add_nodes_from(nodes)\n",
    "    G_train.add_edges_from(training_links)\n",
    "    return G_train\n",
    "\n",
    "\n",
    "def create_training_df(G_train,train_links,missinglinks):\n",
    "    nodes = list(G_train.nodes)\n",
    "    \n",
    "    all_ran_links = itertools.combinations(nodes,2) \n",
    "    all_ran_links = [tuple(sorted(t)) for t in all_ran_links]\n",
    "\n",
    "    links_train = [tuple(sorted(t)) for t in train_links]\n",
    "    non_links = list(set(all_ran_links) - set(links_train))    \n",
    "    \n",
    "    missinglinks = [tuple(sorted(t)) for t in missinglinks]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"non_links\"] = non_links # X''\n",
    "    df[\"Link_present\"] = 0\n",
    "    df.loc[df[\"non_links\"].isin(missinglinks),\"Link_present\"] =1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b86f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cvs(G):\n",
    "    observed_links,observed_missing_links = create_observed_set(G,0.8)\n",
    "    train_links,train_missing_links = create_training_set(observed_links,0.8)\n",
    "    Gho =  G_create(G,observed_links)\n",
    "    Gtr = G_create(G,train_links)\n",
    "\n",
    "    df_ho = create_training_df(Gho,observed_links,observed_missing_links)\n",
    "    df_tr = create_training_df(Gtr,train_links,train_missing_links)\n",
    "\n",
    "\n",
    "    df_ho_top = top_feats(df_ho,Gho,\"non_links\")\n",
    "    df_tr_top = top_feats(df_tr,Gtr,\"non_links\")\n",
    "    feature_set = df_tr_top.columns[1:]\n",
    "\n",
    "\n",
    "    X_train_cv,y_train_cv,X_test_cv,y_test_cv = create_cross_validation(df_tr_top,\"Link_present\")\n",
    "    return X_train_cv,y_train_cv,X_test_cv,y_test_cv,df_ho_top,df_tr_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfb099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats(df,G_top,col):\n",
    "    #jaccard_coefficient\n",
    "    all_edges = list(df[col])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    jacc_coeff_obj = nx.jaccard_coefficient(G_top,all_edges)\n",
    "    jacc_coeff_edges = []\n",
    "    for uu,vv,jj in jacc_coeff_obj:\n",
    "        jacc_coeff_edges.append(jj)\n",
    "        \n",
    "    df[\"score_jaccard\"] = jacc_coeff_edges\n",
    "    \n",
    "    #adamic_adar_index\n",
    "    adamic_adar_coeff = nx.adamic_adar_index(G_top,all_edges)\n",
    "    adamic_adar_score = []\n",
    "    for uu,vv,jj in adamic_adar_coeff:\n",
    "        adamic_adar_score.append(jj)\n",
    "    df[\"adamic_adar_score\"] = adamic_adar_score\n",
    "    \n",
    "    \n",
    "    resource_allocation_index_coeff =nx.resource_allocation_index(G_top,all_edges)\n",
    "    resource_allocation = []\n",
    "    for uu,vv,jj in resource_allocation_index_coeff:\n",
    "        resource_allocation.append(jj)   \n",
    "    df[\"resource_allocation\"] = resource_allocation\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    preferential_obj = nx.preferential_attachment(G_top,all_edges)\n",
    "    preferential_attachment = []\n",
    "    for uu,vv,jj in preferential_obj:\n",
    "        preferential_attachment.append(jj)   \n",
    "    df[\"preferential_attachment\"] = preferential_attachment\n",
    "    \n",
    "    \n",
    "    \n",
    "    #num_of_neigbours\n",
    "    num_of_edges = []\n",
    "    for i in all_edges:\n",
    "        l=list(nx.common_neighbors(G_top, i[0], i[1]))\n",
    "        num_of_edges.append(len(l))\n",
    "    df[\"num_of_neigbours\"] = num_of_edges\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Number of Degrees\n",
    "    degrees = dict(G_top.degree())\n",
    "    degree_1 = []\n",
    "    degree_2 = []\n",
    "\n",
    "    for i,j in all_edges:\n",
    "        degree_1.append(degrees[i])\n",
    "        degree_2.append(degrees[j])\n",
    "    df[\"degree_1\"] = degree_1\n",
    "    df[\"degree_2\"] = degree_2\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Number of triangles\n",
    "    number_of_triangles =nx.triangles(G_top)\n",
    "    tri_1 = []\n",
    "    tri_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        tri_1.append(number_of_triangles[i])\n",
    "        tri_2.append(number_of_triangles[j])\n",
    "    df[\"tri_1\"] = tri_1\n",
    "    df[\"tri_2\"] = tri_2\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Number of triangles\n",
    "    page_rank =nx.pagerank(G_top)\n",
    "    pr_1 = []\n",
    "    pr_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        pr_1.append(page_rank[i])\n",
    "        pr_2.append(page_rank[j])\n",
    "    df[\"pr_1\"] = pr_1\n",
    "    df[\"pr_2\"] = pr_2\n",
    "    \n",
    "    \n",
    "    #Clustering Coefficients\n",
    "    clustering_coeff =nx.clustering(G_top)\n",
    "    clust_1 = []\n",
    "    clust_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        clust_1.append(clustering_coeff[i])\n",
    "        clust_2.append(clustering_coeff[j])\n",
    "    df[\"clust_1\"] = clust_1\n",
    "    df[\"clust_2\"] = clust_2\n",
    "    \n",
    "    \n",
    "    #Average Neigbourhood degree\n",
    "    aveg_neigh_deg =nx.clustering(G_top)\n",
    "    aveg_neigh_deg_1 = []\n",
    "    aveg_neigh_deg_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        aveg_neigh_deg_1.append(aveg_neigh_deg[i])\n",
    "        aveg_neigh_deg_2.append(aveg_neigh_deg[j])\n",
    "    df[\"aveg_neigh_deg_1\"] = aveg_neigh_deg_1\n",
    "    df[\"aveg_neigh_deg_2\"] = aveg_neigh_deg_2\n",
    "    \n",
    "    # Degree centerlaity\n",
    "    \n",
    "    degree_centrality =nx.degree_centrality(G_top)\n",
    "    degree_centrality_1 = []\n",
    "    degree_centrality_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        degree_centrality_1.append(degree_centrality[i])\n",
    "        degree_centrality_2.append(degree_centrality[j])\n",
    "    df[\"degree_centrality_1\"] = degree_centrality_1\n",
    "    df[\"degree_centrality_2\"] = degree_centrality_2\n",
    "    \n",
    "    \"\"\"\n",
    "    #EIgen vector centerality\n",
    "    eigenvector_centrality =nx.eigenvector_centrality(G_top)\n",
    "    eigenvector_centrality_1 = []\n",
    "    eigenvector_centrality_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        eigenvector_centrality_1.append(eigenvector_centrality[i])\n",
    "        eigenvector_centrality_2.append(eigenvector_centrality[j])\n",
    "    df[\"eigenvector_centrality_1\"] = eigenvector_centrality_1\n",
    "    df[\"eigenvector_centrality_2\"] = eigenvector_centrality_2\"\"\"  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Closness centerlaity\n",
    "    closeness_centrality =nx.closeness_centrality(G_top)\n",
    "    closeness_centrality_1 = []\n",
    "    closeness_centrality_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        closeness_centrality_1.append(closeness_centrality[i])\n",
    "        closeness_centrality_2.append(closeness_centrality[j])\n",
    "    df[\"closeness_centrality_1\"] = closeness_centrality_1\n",
    "    df[\"closeness_centrality_2\"] = closeness_centrality_2\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    katz_centrality =nx.katz_centrality(G,tol=1e-3,max_iter=100000)\n",
    "    katz_centrality_1 = []\n",
    "    katz_centrality_2 = []\n",
    "    \n",
    "    for i,j in all_edges:\n",
    "        katz_centrality_1.append(katz_centrality[i])\n",
    "        katz_centrality_2.append(katz_centrality[j])\n",
    "    df[\"katz_centrality_1\"] = katz_centrality_1\n",
    "    df[\"katz_centrality_2\"] = katz_centrality_2\"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4203a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation(top_df_tr,col):\n",
    "    feature_set = ['score_jaccard', 'adamic_adar_score', 'resource_allocation',\n",
    "       'preferential_attachment', 'num_of_neigbours', 'degree_1', 'degree_2',\n",
    "       'tri_1', 'tri_2', 'pr_1', 'pr_2', 'clust_1', 'clust_2',\n",
    "       'aveg_neigh_deg_1', 'aveg_neigh_deg_2', 'degree_centrality_1',\n",
    "       'degree_centrality_2', 'closeness_centrality_1',\n",
    "       'closeness_centrality_2']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    y_train_orig = np.array(top_df_tr[col])\n",
    "    X_train_orig = top_df_tr\n",
    "    \n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    skf.get_n_splits(X_train_orig, y_train_orig)\n",
    "\n",
    "    nFold = 1 \n",
    "\n",
    "    X_train_cv = {}\n",
    "    y_train_cv = {}\n",
    "    X_test_cv = {}\n",
    "    y_test_cv = {}\n",
    "\n",
    "\n",
    "    for train_index, test_index in skf.split(X_train_orig, y_train_orig):\n",
    "\n",
    "        cv_train = list(train_index)\n",
    "        cv_test = list(test_index)\n",
    "\n",
    "\n",
    "        train = X_train_orig.iloc[np.array(cv_train)]\n",
    "        test = X_train_orig.iloc[np.array(cv_test)]\n",
    "\n",
    "        y_train = train[\"Link_present\"]\n",
    "        y_test = test[\"Link_present\"]\n",
    "\n",
    "\n",
    "        X_train = train.loc[:,feature_set]\n",
    "        X_test = test.loc[:,feature_set]\n",
    "\n",
    "        X_test.fillna(X_test.mean(), inplace=True)\n",
    "        X_train.fillna(X_train.mean(), inplace=True)\n",
    "\n",
    "        sm = RandomOverSampler(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        X_train_cv[nFold] = X_train\n",
    "        y_train_cv[nFold] = y_train\n",
    "        X_test_cv[nFold] = X_test\n",
    "        y_test_cv[nFold] = y_test\n",
    "\n",
    "\n",
    "\n",
    "        #print( \"created fold \",nFold, \" ...\")\n",
    "\n",
    "        nFold = nFold + 1\n",
    "    return X_train_cv,y_train_cv,X_test_cv,y_test_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a268c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7c300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cvs(G):\n",
    "    observed_links,observed_missing_links = create_observed_set(G,0.8)\n",
    "    train_links,train_missing_links = create_training_set(observed_links,0.8)\n",
    "    Gho =  G_create(G,observed_links)\n",
    "    Gtr = G_create(G,train_links)\n",
    "\n",
    "    df_ho = create_training_df(Gho,observed_links,observed_missing_links)\n",
    "    df_tr = create_training_df(Gtr,train_links,train_missing_links)\n",
    "\n",
    "\n",
    "    df_ho_top = top_feats(df_ho,Gho,\"non_links\")\n",
    "    df_tr_top = top_feats(df_tr,Gtr,\"non_links\")\n",
    "    feature_set = df_tr_top.columns[1:]\n",
    "\n",
    "\n",
    "    X_train_cv,y_train_cv,X_test_cv,y_test_cv = create_cross_validation(df_tr_top,\"Link_present\")\n",
    "    output_dict = {\n",
    "        'X_train_cv': X_train_cv,\n",
    "        'y_train_cv': y_train_cv,\n",
    "        'X_test_cv': X_test_cv,\n",
    "        'y_test_cv': y_test_cv,\n",
    "    }\n",
    "\n",
    "    \n",
    "    return output_dict,df_ho_top,df_tr_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6394032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "# load the data \n",
    "infile = open('OLP_updated.pickle','rb')  \n",
    "df = pickle.load(infile)  \n",
    "\n",
    "\n",
    "def create_g_from_df(index):\n",
    "    edges_orig  = df[df[\"network_index\"] == index]\n",
    "    edges = edges_orig[\"edges_id\"].values[0]\n",
    "    edges_lis = edges.tolist()\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges_lis)\n",
    "    return  G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0903eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5abf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_index = df[\"network_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f092e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocess import Pool, cpu_count\n",
    "\n",
    "def process_index(index):\n",
    "    index_dir = \"Network_datasest/{}_index\".format(index)\n",
    "    os.mkdir(index_dir)\n",
    "\n",
    "    G = create_g_from_df(index)\n",
    "    output_dict, df_ho_top, df_tr_top = create_cvs(G)\n",
    "\n",
    "    with open(os.path.join(index_dir, 'out.pkl'), 'wb') as f:\n",
    "        pickle.dump((output_dict), f)\n",
    "\n",
    "    df_ho_top.to_csv(os.path.join(index_dir, 'df_ho_top.csv'), index=False)\n",
    "    df_tr_top.to_csv(os.path.join(index_dir, 'df_tr_top.csv'), index=False)\n",
    "\n",
    "    # Assuming you have a list of indices\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149539ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28817c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
